{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code for the master thesis of Job Wegman\n",
    "\n",
    "## this code will be divided in 3 stages\n",
    "\n",
    "## first, the hyper parameters\n",
    "## second, the perparation of the data\n",
    "## third, the models and results\n",
    "\n",
    "## hyperparameters\n",
    "\n",
    "bigram_treshold = 70\n",
    "keyword_number = 50\n",
    "max_feats = 2000\n",
    "k_neigbours = 7\n",
    "bigram_enabled = True \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ruud/Documents/DSS/thesis/JobWegman_Thesis2020\n",
      "there are this many keywords in list1:  50\n",
      "fully loaded\n",
      "['government', 'conspiracy', 'vaccine', 'trump', 'freedom', 'abuse', 'speech', 'china', 'epstein', 'currency', 'federal', 'reptilians', 'woman', 'zuckerberg', 'cia', 'pedophile', 'wealth', 'pyramid', 'allege', 'darpa', 'president', 'lord', 'turkey', 'satanic', 'reptilian', 'council', 'gender', 'mainstream', 'bush', 'nation', 'intelligence', 'censor', 'consciousness', 'allegation', 'society', 'traveler', 'report', 'network', 'equality', 'trust', 'hierarchy', 'civilization', 'facebook', 'jordan', 'mariah', 'social', 'weinstein', 'agendum', 'bible', 'evidence']\n",
      "finished extracting keywords\n"
     ]
    }
   ],
   "source": [
    "### Code for the master thesis of Job Wegman\n",
    "\n",
    "## this code will be divided in 7 stages\n",
    "\n",
    "from functions_job import dataset_shuffler\n",
    "from functions_job import keywordextractor\n",
    "from functions_job import make_bigram\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "#os.chdir('/Users/ruud/Documents/DSS/thesis/JobWegman_Thesis2020')\n",
    "print(os.getcwd())\n",
    "ourwatched_set_filepath = '/Users/ruud/Documents/DSS/thesis/JobWegman_Thesis2020/data'\n",
    "## filepath of: which folder the pickled data should be stored\n",
    "pickle_filepath = '/Users/ruud/Documents/DSS/thesis/JobWegman_Thesis2020/pickles'\n",
    "## filepath of: labels of transcripts from ~480 videos (Alfano, et al.)\n",
    "dataset_filepath = '/Users/ruud/Documents/DSS/thesis/JobWegman_Thesis2020/data'\n",
    "## filepath of: transcripts from ~480 videos from (Alfano, et al.)\n",
    "transcript_filepath = '/Users/ruud/Documents/DSS/thesis/JobWegman_Thesis2020/data'\n",
    "\n",
    "teamA_filepath = '/Users/ruud/Documents/DSS/thesis/JobWegman_Thesis2020'\n",
    "teamB_filepath = '/Users/ruud/Documents/DSS/thesis/JobWegman_Thesis2020'\n",
    "\n",
    "\n",
    "## shuffle datasets ##\n",
    "\n",
    "\n",
    "## The training and test data both consist of the shape ([transcript, conspiracy rating]) ##\n",
    "\n",
    "test_data, training_data = dataset_shuffler(ourwatched_set_filepath, pickle_filepath, dataset_filepath, transcript_filepath, teamA_filepath,  teamB_filepath)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from functions_job import cleaner\n",
    "from functions_job import my_cleaner3\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import en_core_web_sm\n",
    "\n",
    "nlp = en_core_web_sm.load(disable=[\"parser\", \"tagger\", \"ner\"])\n",
    "\n",
    "\n",
    "training_data_cleaned = cleaner(training_data,nlp, pickle_filepath,'training_data_cleaned')\n",
    "test_data_cleaned = cleaner(test_data,nlp, pickle_filepath,'test_data_cleaned')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 2. in this part the bigrams are formed. ------------------------------------\n",
    "\n",
    "if bigram_enabled == True:\n",
    "    training_data_cleaned, test_data_cleaned = make_bigram(training_data_cleaned, test_data_cleaned, bigram_treshold)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "## keywords extraction -------------------\n",
    "\n",
    "filepath = pickle_filepath\n",
    "\n",
    "keywordextractor(filepath,'training_data_cleaned.pickle',keyword_number)\n",
    "\n",
    "import os\n",
    "os.chdir(filepath)\n",
    "import pickle\n",
    "with open(str(keyword_number) + 'keyword_list1.pickle', 'rb') as f:\n",
    "    keyword_list1 = pickle.load(f)\n",
    "with open(str(keyword_number) + 'keyword_list2.pickle', 'rb') as f:\n",
    "    keyword_list2 = pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Feature extraction\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "\n",
    "# for the train set I create X and y\n",
    "X_trainingdata = []\n",
    "y_trainingdata = []\n",
    "for item in training_data_cleaned:\n",
    "    X_trainingdata.append(item[0])\n",
    "    y_trainingdata.append(item[1])\n",
    "\n",
    "# Here I transform X to fit the input of the tfidf vectorizer\n",
    "list_document_tokens = []\n",
    "for i, document in enumerate(X_trainingdata):\n",
    "    list_document_tokens.append(training_data_cleaned[i][0])\n",
    "tfidf_input = []\n",
    "for document in list_document_tokens:\n",
    "    tfidf_input.append(\" \".join(document))\n",
    "\n",
    "\n",
    "# I do the same for my test data\n",
    "X_testdata = []\n",
    "y_testdata = []\n",
    "for item in test_data_cleaned:\n",
    "    X_testdata.append(item[0])\n",
    "    y_testdata.append(item[1])\n",
    "\n",
    "# once again making them fit\n",
    "list_document_tokens_test = []\n",
    "for i, document in enumerate(X_testdata):\n",
    "    list_document_tokens_test.append(test_data_cleaned[i][0])\n",
    "tfidf_input_test = []\n",
    "for document in list_document_tokens_test:\n",
    "    tfidf_input_test.append(\" \".join(document))\n",
    "\n",
    "    \n",
    "### TF IDF VECTORIZATION\n",
    "\n",
    "\n",
    "#for training I fit and transform the train data:\n",
    "tv_training = TfidfVectorizer(stop_words=None, max_features=max_feats)\n",
    "tf_idf_prel_training = tv_training.fit_transform(tfidf_input)\n",
    "tf_idf_vector_training = tf_idf_prel_training.toarray()\n",
    "tf_idf_feature_names_training = tv_training.get_feature_names()\n",
    "\n",
    "#for test, using the same vectorizer I only transform the test data\n",
    "tf_idf_prel_test = tv_training.transform(tfidf_input_test)\n",
    "tf_idf_vector_test = tf_idf_prel_test.toarray()\n",
    "\n",
    "\n",
    "### We route this back in X and Y variables (X_val & y_val for the validation stage, X_ts0 & y_tst for the final test set)\n",
    "#\n",
    "X_val = tf_idf_vector_training\n",
    "y_val = []\n",
    "\n",
    "# Here we condense class 2 & 3 into one class, leaving is with 0 (non conspiratorial) or 1 (conspiratorial)\n",
    "for document in y_trainingdata:\n",
    "    class_made = 0\n",
    "    if document == 1:\n",
    "        class_made = 0\n",
    "    else:\n",
    "        class_made = 1\n",
    "    y_val.append(class_made)\n",
    "\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "X_tst = tf_idf_vector_test\n",
    "y_tst = []\n",
    "for document in y_testdata:\n",
    "\n",
    "    class_made = 0\n",
    "    if document == 1:\n",
    "        class_made = 0\n",
    "    else:\n",
    "        class_made = 1\n",
    "    y_tst.append(class_made)\n",
    "\n",
    "y_tst = np.array(y_tst)\n",
    "\n",
    "\n",
    "\n",
    "### Here we set a baseline for the rest of the research\n",
    "\n",
    "\n",
    "# train test split for validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_val, y_val, test_size=0.2, random_state=0)\n",
    "\n",
    "X_bias1 = []\n",
    "\n",
    "# we need gensims word2vec here\n",
    "import gensim\n",
    "import logging\n",
    "\n",
    "# initialise and train the Word2Vec model\n",
    "model = gensim.models.Word2Vec(list_document_tokens, size=150, window=10, min_count=0, workers=1, iter=10, seed = 0)\n",
    "\n",
    "\n",
    "for w1 in tf_idf_feature_names_training:\n",
    "    if w1 != 'pron' and w1 != 'to':\n",
    "        counter = 0\n",
    "        averager = 0\n",
    "        for w2 in keyword_list1:\n",
    "            \n",
    "            try:\n",
    "                counter += model.wv.similarity(w1, w2)\n",
    "                #print(counter)\n",
    "                averager += 1\n",
    "            except:\n",
    "                averager += 0\n",
    "                pass\n",
    "\n",
    "        #counter = counter/averager\n",
    "        if counter > 0:\n",
    "            counter += 1\n",
    "        else:\n",
    "            counter = counter\n",
    "        X_bias1.append(counter)\n",
    "    else:\n",
    "        counter = 0\n",
    "        X_bias1.append(counter)   \n",
    "        \n",
    "\n",
    "X_val_biased_1 = []\n",
    "for i,matrixrow in enumerate(X_val):\n",
    "    temp_num = []\n",
    "    for j, item in enumerate(matrixrow):\n",
    "\n",
    "        temp_num.append(X_bias1[j]*item)\n",
    "\n",
    "    X_val_biased_1.append(temp_num)\n",
    "import numpy as np\n",
    "X_val_biased_1 = np.array(X_val_biased_1)\n",
    "\n",
    "X_test_biased_1 = []\n",
    "for i,matrixrow in enumerate(X_tst):\n",
    "    temp_num = []\n",
    "    for j, item in enumerate(matrixrow):\n",
    "\n",
    "        temp_num.append(X_bias1[j]*item)\n",
    "\n",
    "    X_test_biased_1.append(temp_num)\n",
    "import numpy as np\n",
    "X_test_biased_1 = np.array(X_test_biased_1)\n",
    "\n",
    "\n",
    "# Now I route this back in X and Y variables\n",
    "#for training:\n",
    "\n",
    "\n",
    "X_val_bias1 = X_val_biased_1\n",
    "y_val_bias1 = []\n",
    "for document in y_trainingdata:\n",
    "\n",
    "    class_made = 0\n",
    "    if document == 1:\n",
    "        class_made = 0\n",
    "    else:\n",
    "        class_made = 1\n",
    "    y_val_bias1.append(class_made)\n",
    "\n",
    "y_val_bias1 = np.array(y_val_bias1)\n",
    "\n",
    "# for testing\n",
    "X_tst_bias1 = X_test_biased_1\n",
    "y_tst_bias1 = []\n",
    "for document in y_testdata:\n",
    "\n",
    "    class_made = 0\n",
    "    if document == 1:\n",
    "        class_made = 0\n",
    "    else:\n",
    "        class_made = 1\n",
    "    y_tst_bias1.append(class_made)\n",
    "\n",
    "y_tst_bias1 = np.array(y_tst_bias1)\n",
    "\n",
    "\n",
    "# train test split for validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_val_bias1, y_val_bias1, test_size=0.2, random_state=0)\n",
    "\n",
    "#print(len(X_train) + len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "euc val bigram\n",
      "[[44  6]\n",
      " [ 9 17]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.88      0.85        50\n",
      "           1       0.74      0.65      0.69        26\n",
      "\n",
      "    accuracy                           0.80        76\n",
      "   macro avg       0.78      0.77      0.77        76\n",
      "weighted avg       0.80      0.80      0.80        76\n",
      "\n",
      "0.8026315789473685\n",
      "0.77\n",
      "euc test bigram\n",
      "[[62  7]\n",
      " [11 16]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.90      0.87        69\n",
      "           1       0.70      0.59      0.64        27\n",
      "\n",
      "    accuracy                           0.81        96\n",
      "   macro avg       0.77      0.75      0.76        96\n",
      "weighted avg       0.81      0.81      0.81        96\n",
      "\n",
      "0.8125\n",
      "0.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cos val bigram\n",
      "[[44  6]\n",
      " [ 7 19]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.88      0.87        50\n",
      "           1       0.76      0.73      0.75        26\n",
      "\n",
      "    accuracy                           0.83        76\n",
      "   macro avg       0.81      0.81      0.81        76\n",
      "weighted avg       0.83      0.83      0.83        76\n",
      "\n",
      "0.8289473684210527\n",
      "0.81\n",
      "cos test bigram\n",
      "[[57 12]\n",
      " [ 5 22]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.83      0.87        69\n",
      "           1       0.65      0.81      0.72        27\n",
      "\n",
      "    accuracy                           0.82        96\n",
      "   macro avg       0.78      0.82      0.80        96\n",
      "weighted avg       0.84      0.82      0.83        96\n",
      "\n",
      "0.8229166666666666\n",
      "0.80\n",
      "dwc val bigram\n",
      "[[40 10]\n",
      " [ 5 21]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.80      0.84        50\n",
      "           1       0.68      0.81      0.74        26\n",
      "\n",
      "    accuracy                           0.80        76\n",
      "   macro avg       0.78      0.80      0.79        76\n",
      "weighted avg       0.82      0.80      0.81        76\n",
      "\n",
      "0.8026315789473685\n",
      "0.79\n",
      "DWC test bigram\n"
     ]
    }
   ],
   "source": [
    "## model 1: euclidean distance\n",
    "#k_neigbours = 7\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "euc = KNeighborsClassifier(metric ='euclidean', p=2, n_neighbors= k_neigbours)\n",
    "euc.fit(X_train, y_train)\n",
    "\n",
    "# validation EUC\n",
    "\n",
    "if bigram_enabled == True:\n",
    "    print(\"euc val bigram\")\n",
    "else:\n",
    "    print(\"euc val\")\n",
    "\n",
    "\n",
    "y_pred_euc = euc.predict(X_test)\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(confusion_matrix(y_test, y_pred_euc))\n",
    "print(classification_report(y_test, y_pred_euc))\n",
    "print(accuracy_score(y_test, y_pred_euc))\n",
    "\n",
    "\n",
    "\n",
    "if bigram_enabled == True:\n",
    "    print(\"euc test bigram\")\n",
    "else:\n",
    "    print(\"euc test\")\n",
    "y_pred_euc_test = euc.predict(X_tst_bias1)\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(confusion_matrix(y_tst_bias1, y_pred_euc_test))\n",
    "print(classification_report(y_tst_bias1, y_pred_euc_test))\n",
    "print(accuracy_score(y_tst_bias1, y_pred_euc_test))\n",
    "\n",
    "\n",
    "\n",
    "## model 2: cosine distance\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import datasets, metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from keras.datasets import mnist\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.externals import joblib\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "cos = KNeighborsClassifier(metric ='cosine', p=2, n_neighbors= k_neigbours)\n",
    "cos.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# validation COS\n",
    "\n",
    "if bigram_enabled == True:\n",
    "    print(\"cos val bigram\")\n",
    "else:\n",
    "    print(\"cos val\")\n",
    "y_pred_cos = cos.predict(X_test)\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(confusion_matrix(y_test, y_pred_cos))\n",
    "print(classification_report(y_test, y_pred_cos))\n",
    "print(accuracy_score(y_test, y_pred_cos))\n",
    "\n",
    "\n",
    "if bigram_enabled == True:\n",
    "    print(\"cos test bigram\")\n",
    "else:\n",
    "    print(\"cos test\")\n",
    "y_pred_cos_test = cos.predict(X_tst_bias1)\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(confusion_matrix(y_tst_bias1, y_pred_cos_test))\n",
    "print(classification_report(y_tst_bias1, y_pred_cos_test))\n",
    "print(accuracy_score(y_tst_bias1, y_pred_cos_test))\n",
    "\n",
    "\n",
    "\n",
    "## model 3: DWC\n",
    "\n",
    "from scipy.spatial import distance\n",
    "\n",
    "\n",
    "def DWC_distance(string1, string2): \n",
    "    \n",
    "    cossim = 1 - distance.cosine(string1, string2)\n",
    "    distance1 = 0\n",
    "    \n",
    "    L = len(string1)\n",
    "    for i in range(L):\n",
    "        \n",
    "        if np.sign([string1[i]]) != np.sign([string2[i]]):\n",
    "            if string1[i] == 0 and string2[i] == 0:\n",
    "                distance1 += 0\n",
    "            elif string1[i] == 0:\n",
    "                distance1 += string2[i]\n",
    "            elif string2[i] == 0:\n",
    "                distance1 += string1[i]\n",
    "            else:\n",
    "                distance1 += string1[i] * string2[i]\n",
    "                \n",
    "                \n",
    "    \n",
    "    return 1-(cossim / (cossim + (distance1/L) ))\n",
    "\n",
    "DWC = KNeighborsClassifier(metric = DWC_distance, p=2, n_neighbors= k_neigbours)\n",
    "\n",
    "DWC.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# validation DWC\n",
    "\n",
    "if bigram_enabled == True:\n",
    "    print(\"dwc val bigram\")\n",
    "else:\n",
    "    print(\"dwc val\")\n",
    "\n",
    "y_pred_DWC = DWC.predict(X_test)\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(confusion_matrix(y_test, y_pred_DWC))\n",
    "print(classification_report(y_test, y_pred_DWC))\n",
    "print(accuracy_score(y_test, y_pred_DWC))\n",
    "\n",
    "\n",
    "if bigram_enabled == True:\n",
    "    print(\"DWC test bigram\")\n",
    "else:\n",
    "    print(\"DWC test\")\n",
    "y_pred_DWC_test = DWC.predict(X_tst_bias1)\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(confusion_matrix(y_tst_bias1, y_pred_DWC_test))\n",
    "print(classification_report(y_tst_bias1, y_pred_DWC_test))\n",
    "print(accuracy_score(y_tst_bias1, y_pred_DWC_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
